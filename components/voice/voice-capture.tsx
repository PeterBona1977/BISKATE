"use client"

import { useState, useEffect, useRef } from "react"
import { Button } from "@/components/ui/button"
import { Dialog, DialogContent, DialogDescription, DialogHeader, DialogTitle } from "@/components/ui/dialog"
import { Textarea } from "@/components/ui/textarea"
import { Label } from "@/components/ui/label"
import { Mic, MicOff, Volume2, AlertCircle } from "lucide-react"
import { cn } from "@/lib/utils"
import { Alert, AlertDescription } from "@/components/ui/alert"

// Defini√ß√£o de tipo local para SpeechRecognition
interface SpeechRecognitionErrorEvent extends Event {
  error: string
  message: string
}

interface SpeechRecognitionEvent extends Event {
  resultIndex: number
  results: SpeechRecognitionResultList
}

interface VoiceCaptureProps {
  onVoiceProcessed: (extractedData: {
    title: string
    description: string
    category: string
    price: number
    location: string
    estimatedTime: string
  }) => void
  isOpen: boolean
  onClose: () => void
}

export function VoiceCapture({ onVoiceProcessed, isOpen, onClose }: VoiceCaptureProps) {
  const [isRecording, setIsRecording] = useState(false)
  const [voiceText, setVoiceText] = useState("")
  const [isProcessing, setIsProcessing] = useState(false)
  const [error, setError] = useState<string | null>(null)
  const [permissionStatus, setPermissionStatus] = useState<PermissionState | null>(null)
  const [debugLogs, setDebugLogs] = useState<string[]>([])

  // Audio Input State
  const [audioDevices, setAudioDevices] = useState<MediaDeviceInfo[]>([])
  const [selectedDeviceId, setSelectedDeviceId] = useState<string>("")
  const [audioLevel, setAudioLevel] = useState<number>(0)
  const audioContextRef = useRef<AudioContext | null>(null)
  const sourceRef = useRef<MediaStreamAudioSourceNode | null>(null)
  const analyserRef = useRef<AnalyserNode | null>(null)
  const animationFrameRef = useRef<number | null>(null)

  const addDebugLog = (msg: string) => {
    setDebugLogs(prev => [`[${new Date().toLocaleTimeString()}] ${msg}`, ...prev].slice(0, 10))
  }

  // Listar dispositivos
  const enumerateDevices = async () => {
    try {
      const devices = await navigator.mediaDevices.enumerateDevices()
      const audioInputs = devices.filter(d => d.kind === 'audioinput')
      setAudioDevices(audioInputs)
      // Se j√° tivermos um selecionado, mantemos. Se n√£o, pegamos o padr√£o.
      if (!selectedDeviceId && audioInputs.length > 0) {
        // Tenta achar o 'default' ou pega o primeiro
        const defaultDevice = audioInputs.find(d => d.deviceId === 'default') || audioInputs[0]
        setSelectedDeviceId(defaultDevice.deviceId)
      }
      addDebugLog(`Dispositivos encontrados: ${audioInputs.length}`)
    } catch (err) {
      console.error("Erro ao listar dispositivos", err)
      addDebugLog("Erro ao listar dispositivos de √°udio")
    }
  }

  // Inicializar o monitoramento de √°udio (Visualizador)
  const startAudioVisualizer = async () => {
    try {
      if (!audioContextRef.current) {
        audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)()
      }

      const constraints = {
        audio: selectedDeviceId ? { deviceId: { exact: selectedDeviceId } } : true
      }

      const stream = await navigator.mediaDevices.getUserMedia(constraints)

      const analyser = audioContextRef.current.createAnalyser()
      analyser.fftSize = 256
      const source = audioContextRef.current.createMediaStreamSource(stream)
      source.connect(analyser)

      analyserRef.current = analyser
      sourceRef.current = source

      const dataArray = new Uint8Array(analyser.frequencyBinCount)

      const updateLevel = () => {
        if (!analyserRef.current) return
        analyserRef.current.getByteFrequencyData(dataArray)

        // M√©dia simples do volume
        let sum = 0
        for (let i = 0; i < dataArray.length; i++) {
          sum += dataArray[i]
        }
        const average = sum / dataArray.length
        setAudioLevel(average / 128) // Normaliza 0-2 (aproximadamente)

        animationFrameRef.current = requestAnimationFrame(updateLevel)
      }

      updateLevel()
    } catch (err) {
      console.error("Erro ao iniciar visualizador de √°udio", err)
    }
  }

  const stopAudioVisualizer = () => {
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current)
    }
    if (sourceRef.current) {
      sourceRef.current.disconnect()
      // Importante: parar as tracks do stream para soltar o microfone
      if (sourceRef.current.mediaStream) {
        sourceRef.current.mediaStream.getTracks().forEach(t => t.stop())
      }
    }
    if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
      audioContextRef.current.close()
      audioContextRef.current = null
    }
    setAudioLevel(0)
  }

  // Refer√™ncias para o reconhecimento de voz
  const recognitionRef = useRef<any>(null)
  const textAreaRef = useRef<HTMLTextAreaElement>(null)

  const categories = [
    { keywords: ["limpar", "limpeza", "aspirar", "varrer", "esfregar"], category: "Limpeza" },
    { keywords: ["jardim", "jardinagem", "podar", "plantar", "regar", "relva"], category: "Jardinagem" },
    {
      keywords: ["reparar", "consertar", "montar", "arm√°rio", "m√≥vel", "estante", "mesa", "cadeira", "ikea"],
      category: "Repara√ß√µes",
    },
    { keywords: ["transportar", "levar", "buscar", "mudan√ßa", "carregar"], category: "Transporte" },
    { keywords: ["cozinhar", "comida", "refei√ß√£o", "jantar", "almo√ßo"], category: "Cozinha" },
    { keywords: ["computador", "pc", "laptop", "instalar", "configurar", "wifi"], category: "Tecnologia" },
    { keywords: ["c√£o", "gato", "animal", "passear", "pet", "veterin√°rio"], category: "Animais" },
  ]

  // Inicializar o reconhecimento de voz
  useEffect(() => {
    // Verificar se o navegador suporta a API de reconhecimento de voz
    if (!("webkitSpeechRecognition" in window) && !("SpeechRecognition" in window)) {
      setError("O seu navegador n√£o suporta reconhecimento de voz. Tente usar Chrome, Edge ou Safari.")
      return
    }

    // Verificar permiss√£o do microfone
    if (navigator.permissions && navigator.permissions.query) {
      navigator.permissions
        .query({ name: "microphone" as PermissionName })
        .then((status) => {
          setPermissionStatus(status.state)
          if (status.state === 'granted') {
            enumerateDevices()
          }

          status.onchange = () => {
            setPermissionStatus(status.state)
            if (status.state === 'granted') {
              enumerateDevices()
            }
          }
        })
        .catch((err) => {
          console.error("Erro ao verificar permiss√£o:", err)
        })
    } else {
      // Fallback for browsers without permissions API
      enumerateDevices()
    }

    // Limpar ao desmontar
    return () => {
      stopAudioVisualizer()
      if (recognitionRef.current) {
        recognitionRef.current.onresult = null
        recognitionRef.current.onend = null
        recognitionRef.current.onerror = null
        recognitionRef.current.abort()
        recognitionRef.current = null
      }
    }
  }, [])

  const extractDataFromText = (text: string) => {
    // Safety check: ensure text is defined and is a string
    if (!text || typeof text !== 'string') {
      console.warn('extractDataFromText called with invalid text:', text)
      return {
        title: "Servi√ßo solicitado",
        description: "",
        category: "Outros",
        price: 0,
        location: "",
        estimatedTime: "2-3 horas",
      }
    }

    const lowerText = text.toLowerCase()

    // Extrair pre√ßo
    const priceMatch = text.match(/(\d+(?:[.,]\d+)?)\s*(?:euros?|‚Ç¨|eur)/i)
    const price = priceMatch ? Number.parseFloat(priceMatch[1].replace(",", ".")) : 0

    // Extrair localiza√ß√£o (procurar por cidades portuguesas comuns e padr√µes)
    const locationPatterns = [
      /(?:em|de|para|na|no)\s+([A-Z√Å√Ä√Ç√É√â√ä√ç√ì√î√ï√ö√á][a-z√°√†√¢√£√©√™√≠√≥√¥√µ√∫√ß\s]+?)(?:\s|,|$)/i,
      /(lisboa|porto|coimbra|braga|aveiro|faro|set√∫bal|√©vora|viseu|leiria|santar√©m|bragan√ßa|castelo branco|guarda|portalegre|viana do castelo|vila real|funchal|angra do hero√≠smo)/i,
    ]

    let location = ""
    for (const pattern of locationPatterns) {
      const match = text.match(pattern)
      if (match) {
        location = match[1] || match[0]
        location = location.replace(/^(em|de|para|na|no)\s+/i, "").trim()
        break
      }
    }

    // Extrair categoria baseada em palavras-chave
    let category = "Outros"
    for (const cat of categories) {
      if (cat.keywords.some((keyword) => lowerText.includes(keyword))) {
        category = cat.category
        break
      }
    }

    // Extrair t√≠tulo (primeira parte da frase, geralmente o servi√ßo)
    let title = ""
    const titlePatterns = [
      /(?:preciso|quero|necessito)\s+(?:de\s+)?(?:algu√©m\s+para\s+)?(.+?)(?:\s*,|\s+pago|\s+por|\s+em|\s+‚Ç¨|\s+euros?|$)/i,
      /(.+?)(?:\s*,|\s+pago|\s+por|\s+em|\s+‚Ç¨|\s+euros?)/i,
    ]

    for (const pattern of titlePatterns) {
      const match = text.match(pattern)
      if (match && match[1]) {
        title = match[1].trim()
        // Limpar palavras desnecess√°rias do in√≠cio
        title = title.replace(/^(preciso de|quero|necessito|algu√©m para|para)/i, "").trim()
        break
      }
    }

    // Se n√£o encontrou t√≠tulo, usar as primeiras palavras
    if (!title) {
      const words = text.split(" ")
      title = words.slice(0, Math.min(5, words.length)).join(" ")
    }

    // Capitalizar primeira letra do t√≠tulo
    title = title.charAt(0).toUpperCase() + title.slice(1)

    // Estimar tempo baseado no tipo de servi√ßo
    let estimatedTime = "2-3 horas"
    if (lowerText.includes("limpar") || lowerText.includes("limpeza")) {
      estimatedTime = "3-4 horas"
    } else if (lowerText.includes("montar") || lowerText.includes("m√≥vel")) {
      estimatedTime = "1-2 horas"
    } else if (lowerText.includes("jardim") || lowerText.includes("jardinagem")) {
      estimatedTime = "2-4 horas"
    } else if (lowerText.includes("passear") || lowerText.includes("c√£o")) {
      estimatedTime = "1 hora"
    }

    return {
      title: title || "Servi√ßo solicitado",
      description: text,
      category,
      price: price || 0,
      location: location || "",
      estimatedTime,
    }
  }

  const handleStartRecording = async () => {
    setError(null)

    try {
      // Verificar se o navegador suporta a API
      const SpeechRecognition = window.SpeechRecognition || (window as any).webkitSpeechRecognition
      if (!SpeechRecognition) {
        // Detec√ß√£o espec√≠fica para Firefox para mensagem mais √∫til
        if (navigator.userAgent.indexOf("Firefox") !== -1) {
          throw new Error("O Firefox n√£o suporta reconhecimento de voz nativo. Por favor, use Chrome ou Edge.")
        }
        throw new Error("Reconhecimento de voz n√£o suportado neste navegador.")
      }

      // Parar inst√¢ncia anterior se existir
      if (recognitionRef.current) {
        recognitionRef.current.abort()
      }

      // Solicitar permiss√£o do microfone se necess√°rio
      // Nota: Edge √†s vezes precisa de um getUserMedia expl√≠cito para "acordar" a permiss√£o
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
        // Mantemos as tracks ativas por um momento para garantir que o navegador registre o uso
        setTimeout(() => {
          stream.getTracks().forEach((track) => track.stop())
        }, 500)
      } catch (permErr) {
        console.error("Permiss√£o de microfone negada:", permErr)
        setError("Permiss√£o de microfone negada. Por favor, permita o acesso ao microfone nas configura√ß√µes do navegador.")
        return
      }

      // Inicializar o reconhecimento de voz
      const recognition = new SpeechRecognition()
      recognitionRef.current = recognition

      // Configurar o reconhecimento
      recognition.lang = "pt-PT" // Portugu√™s de Portugal

      // Edge fix: 'continuous' e 'interimResults' causam instabilidade no Edge.
      // Desativamos para garantir funcionalidade b√°sica.
      const isEdge = navigator.userAgent.indexOf("Edg") !== -1

      if (isEdge) {
        console.log("Edge detectado: Usando modo de compatibilidade (sem cont√≠nuo/interim)")
        recognition.continuous = false
        recognition.interimResults = false
      } else {
        recognition.continuous = true
        recognition.interimResults = true
      }

      recognition.maxAlternatives = 1

      // Manipular resultados
      recognition.onresult = (event: any) => {
        let transcript = ""
        let isFinal = false

        for (let i = event.resultIndex; i < event.results.length; i++) {
          transcript += event.results[i][0].transcript
          isFinal = event.results[i].isFinal
        }

        if (transcript) {
          addDebugLog(`Resultado parcial: "${transcript.substring(0, 20)}..." (Final: ${isFinal})`)
        }

        setVoiceText((prev) => {
          // Se for resultado final, adicionar ao texto existente
          if (isFinal) {
            return prev + " " + transcript
          }
          // Se for resultado parcial, mostrar no textarea mas n√£o adicionar ao texto final
          if (textAreaRef.current) {
            textAreaRef.current.value = prev + " " + transcript
          }
          return prev
        })
      }

      // Manipular in√≠cio
      recognition.onstart = () => {
        console.log("Reconhecimento de voz iniciado")
        addDebugLog("Evento: onstart (Voz iniciada)")
        setIsRecording(true)
        setError(null)
      }

      // Manipular fim do reconhecimento
      recognition.onend = () => {
        console.log("Reconhecimento de voz terminou")
        addDebugLog("Evento: onend (Voz terminou)")
        // Edge √†s vezes termina prematuramente. S√≥ paramos se o usu√°rio pediu ou se houve erro fatal.
        // Verificamos o estado 'isRecording' (que √© gerenciado pelo React, cuidado com stale closures aqui)
        // Mas como onend √© closure, precisamos usar a ref ou l√≥gica externa se quisermos reiniciar.
        // Simplifica√ß√£o: Se cair, o usu√°rio clica de novo, mas logs ajudam a debug.
        setIsRecording(false)
      }

      // Manipular erros
      recognition.onerror = (event: any) => {
        console.error("Erro de reconhecimento de voz:", event.error)
        addDebugLog(`Evento: onerror (${event.error})`)

        // Ignorar erros "no-speech" ou "aborted" que s√£o comuns e n√£o requerem feedback visual agressivo
        if (event.error === 'no-speech') {
          console.warn("Nenhuma fala detectada. Ignorando erro.")
          return
        }
        if (event.error === 'aborted') {
          console.warn("Reconhecimento abortado. Ignorando erro.")
          return
        }

        setError(`Erro no reconhecimento de voz: ${event.error}`)
        setIsRecording(false)
      }

      // Iniciar o reconhecimento
      addDebugLog("Chamando recognition.start()...")
      try {
        recognition.start()
        setVoiceText("") // Limpar texto anterior ao come√ßar nova grava√ß√£o
      } catch (startErr) {
        addDebugLog(`Erro ao chamar start(): ${startErr}`)
        throw startErr
      }
    } catch (err) {
      console.error("Erro ao iniciar reconhecimento de voz:", err)
      setError(
        `N√£o foi poss√≠vel acessar o microfone. ${err instanceof Error ? err.message : "Verifique as permiss√µes do navegador."}`,
      )
      setIsRecording(false)
    }
  }

  const handleStopRecording = () => {
    if (recognitionRef.current) {
      recognitionRef.current.stop()
    }
    setIsRecording(false)
  }

  const handleProcessVoice = async () => {
    if (!voiceText.trim()) return

    setIsProcessing(true)

    try {
      // Processar o texto capturado
      const extractedData = extractDataFromText(voiceText)

      // Simular um pequeno atraso para processamento
      await new Promise((resolve) => setTimeout(resolve, 1000))

      // Enviar os dados processados
      onVoiceProcessed(extractedData)
      onClose()
      setVoiceText("")
      setIsRecording(false)
    } catch (err) {
      console.error("Erro ao processar voz:", err)
      setError("Ocorreu um erro ao processar o texto. Por favor, tente novamente.")
    } finally {
      setIsProcessing(false)
    }
  }

  const handleCancel = () => {
    if (recognitionRef.current) {
      recognitionRef.current.stop()
    }
    setVoiceText("")
    setIsRecording(false)
    setError(null)
    onClose()
  }

  return (
    <Dialog open={isOpen} onOpenChange={onClose}>
      <DialogContent className="sm:max-w-md">
        <DialogHeader>
          <DialogTitle className="flex items-center gap-2">
            <Mic className="h-5 w-5 text-indigo-600" />
            Criar Gig por Voz
          </DialogTitle>
          <DialogDescription>Descreva o servi√ßo que precisa, incluindo o pre√ßo e localiza√ß√£o</DialogDescription>
        </DialogHeader>

        <div className="space-y-4">
          {/* Mensagem de erro */}
          {error && (
            <Alert variant="destructive" className="text-sm">
              <AlertCircle className="h-4 w-4" />
              <AlertDescription>{error}</AlertDescription>
            </Alert>
          )}

          {/* Seletor de Microfone */}
          <div className="flex flex-col space-y-2">
            <Label htmlFor="mic-select" className="text-xs text-gray-500">Microfone</Label>
            <select
              id="mic-select"
              className="w-full text-sm border rounded p-1.5 bg-white"
              value={selectedDeviceId}
              onChange={(e) => setSelectedDeviceId(e.target.value)}
              disabled={isRecording}
            >
              {audioDevices.length === 0 && <option value="">Padr√£o</option>}
              {audioDevices.map((device) => (
                <option key={device.deviceId} value={device.deviceId}>
                  {device.label || `Microfone ${device.deviceId.slice(0, 5)}...`}
                </option>
              ))}
            </select>
            <p className="text-xs text-blue-500 cursor-pointer hover:underline" onClick={enumerateDevices}>
              üîÑ Atualizar lista
            </p>
          </div>

          {/* Indicador de grava√ß√£o */}
          <div className="flex flex-col items-center space-y-4 py-6">
            <div
              className={cn(
                "relative flex items-center justify-center w-20 h-20 rounded-full transition-all duration-300",
                isRecording
                  ? "bg-red-100 border-4 border-red-300" // Removido animate-pulse para usar visualizador real
                  : "bg-gray-100 border-4 border-gray-300",
              )}
              style={{
                transform: isRecording ? `scale(${1 + Math.min(audioLevel * 2, 0.5)})` : 'scale(1)',
                boxShadow: isRecording ? `0 0 ${audioLevel * 50}px rgba(239, 68, 68, 0.6)` : 'none'
              }}
            >
              {isRecording ? <MicOff className="h-8 w-8 text-red-600" /> : <Mic className="h-8 w-8 text-gray-600" />}
            </div>

            {isRecording && (
              <div className="flex flex-col items-center space-y-1">
                <div className="flex items-center space-x-2 text-red-600">
                  <Volume2 className="h-4 w-4" />
                  <span className="text-sm font-medium">A ouvir...</span>
                </div>
                {/* Barra de volume visual */}
                <div className="w-32 h-2 bg-gray-200 rounded-full overflow-hidden">
                  <div
                    className="h-full bg-green-500 transition-all duration-100"
                    style={{ width: `${Math.min(audioLevel * 100 * 3, 100)}%` }}
                  />
                </div>
              </div>
            )}

            <p className="text-sm text-gray-600 text-center max-w-xs">
              {isRecording
                ? "Diga algo como: 'Preciso de algu√©m para limpar o meu apartamento, pago 45 euros em Lisboa'"
                : "Clique para come√ßar a gravar o seu pedido"}
            </p>
          </div>

          {/* Texto reconhecido */}
          <div className="space-y-2">
            <Label htmlFor="voice-text">Texto reconhecido:</Label>
            <Textarea
              ref={textAreaRef}
              id="voice-text"
              placeholder="O texto reconhecido aparecer√° aqui..."
              value={voiceText}
              onChange={(e) => setVoiceText(e.target.value)}
              rows={4}
              className="resize-none"
              readOnly={isRecording}
            />
            {isRecording && (
              <p className="text-xs text-gray-500 italic">
                Fale claramente. O texto ser√° atualizado automaticamente enquanto fala.
              </p>
            )}
          </div>

          {/* Bot√µes de a√ß√£o */}
          <div className="flex justify-center space-x-3">
            {!isRecording ? (
              <>
                <Button onClick={handleStartRecording} className="flex items-center gap-2">
                  <Mic className="h-4 w-4" />
                  Come√ßar Grava√ß√£o
                </Button>
                <Button variant="outline" onClick={handleCancel}>
                  Cancelar
                </Button>
              </>
            ) : (
              <>
                <Button onClick={handleStopRecording} variant="destructive" className="flex items-center gap-2">
                  <MicOff className="h-4 w-4" />
                  Parar Grava√ß√£o
                </Button>
                <Button
                  onClick={handleProcessVoice}
                  disabled={!voiceText.trim() || isProcessing}
                  className="flex items-center gap-2"
                >
                  {isProcessing ? (
                    <>
                      <div className="h-4 w-4 animate-spin rounded-full border-2 border-white border-t-transparent" />
                      Processando...
                    </>
                  ) : (
                    <>
                      <Volume2 className="h-4 w-4" />
                      Processar Voz
                    </>
                  )}
                </Button>
              </>
            )}
          </div>

          {/* √Årea de Debug (Vis√≠vel apenas se houver logs) */}
          {debugLogs.length > 0 && (
            <div className="mt-4 p-2 bg-gray-100 rounded text-xs font-mono max-h-32 overflow-y-auto border border-gray-200">
              <p className="font-bold text-gray-700 mb-1">Debug Logs:</p>
              {debugLogs.map((log, i) => (
                <div key={i} className="text-gray-600 border-b border-gray-200 last:border-0 py-0.5">
                  {log}
                </div>
              ))}
            </div>
          )}
        </div>
      </DialogContent>
    </Dialog>
  )
}
